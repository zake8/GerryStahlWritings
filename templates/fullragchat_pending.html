	<!DOCTYPE html>
<html lang="en">
	<head>
		<meta http-equiv="content-type" content="text/html; charset=windows-1252">
		<meta http-equiv="refresh" content="0; URL=/fullragchat_reply">
		<!-- first java script! -->
		<script>
			window.onload = function() {
				spot.scrollIntoView();  // Scroll to the element
			};
		</script>
	</head>
	<body>
	<h1>
		<!-- should auto jump to "/fullragchat_reply" -->
		{% for message in fullragchat_history %}
			<font color="purple">{{message.user }}:</font> {{ message.message }}<br>
		{% endfor %}
	</h1>
	<a id="spot"></a>
	<form action="/fullragchat_pending" method="post">

		<!-- query text box with no click to submit while inferencing -->
		<textarea rows="8" cols="120"></textarea><br>
		<!-- query text box with no click to submit while inferencing -->

		<input type="text" name="model" value="{{fullragchat_model}}">
		<lable for="model"><strong>LLM model</strong></lable> - 
		<font color="purple">fake_llm; <i>local on ollama:</i> orca-mini, phi, tinyllama, llama2, llama2-uncensored, mistral, mixtral, command-r; <i>api call:</i> open-mixtral-8x7b, mistral-large-latest</font>
		<br>

		<input type="text" name="temp" value="{{fullragchat_temp}}">
		<lable for="temp"><strong>LLM temperature</strong></lable> - 
		<font color="purple">0.00, more deterministic, to 1.00, more creative</font>
		<br>

		<input type="text" name="rag_source" value="{{fullragchat_rag_source}}">
		<lable for="rag_source"><strong>rag_source</strong></lable> - 
		<font color="purple">Set to Auto to do double LLM passes, first for RAG, then for query; path filename for single faiss, website, pdf, or text file</font>
		<br>

		<input type="text" name="embed_model" value="{{fullragchat_embed_model}}">
		<lable for="model"><strong>embeding model</strong></lable> - <font color="purple"><i>local on ollama:</i> nomic-embed-text; <i>api call:</i> mistral-embed; embeder needs to match LLM</font>
		<br>

		<input type="text" name="stop_words" value="{{fullragchat_stop_words}}">
		<lable for="stop_words"><strong>stop_words</strong></lable> - <font color="purple">words, to stop, processing</font>
		<br>

		<input type="text" name="skin" value="{{fullragchat_skin}}">
		<lable for="skin"><strong>skin</strong></lable>
		<br>

		<input type="text" name="music" value="{{fullragchat_music}}">
		<lable for="music"><strong>music</strong></lable>
		<br>

	</form>
	<a href="/reset_history">reset</a><br>
	chatbot_command.summary(pfn)        # Summarize one file from web or local, output to chat only<br>
	chatbot_command.listclues(None)     # Lists available docs as per clues file<br>
	chatbot_command.listfiles(None)     # Lists available docs from local<br>
	chatbot_command.delete(pfn)         # Not yet implemented (use ssh and winscp)<br>
	chatbot_command.download(pfn)       # Save one file from web to local<br>
	chatbot_command.injest(pfn)         # Injest one file from local<br>
	chatbot_command.batchinjestpdf(pfn) # Batch injest local from based on contents of pfn, see pdf_batch_injest_sample.txt<br>
	Note: docs/rag_source_clues.txt controls rag selection; download to server, then injest.<br>
	</body>
</html>
